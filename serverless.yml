service: llama-apy

package:
  individually: true

frameworkVersion: '3'

provider:
  name: aws
  runtime: nodejs14.x
  stage: dev
  region: eu-central-1
  tracing:
    apiGateway: true
    lambda: true
  memorySize: 256
  iam:
    role:
      statements:
        - Effect: Allow
          Action:
            - xray:PutTraceSegments
            - xray:PutTelemetryRecords
          Resource: '*'
        - Effect: Allow
          Action:
            - s3:ListBucket
            - s3:*Object*
            - sqs:SendMessage
          Resource: '*'
        - Effect: Allow
          Action:
            - ssm:PutParameter
            - ssm:GetParameter
            - ssm:DescribeParameters
            - kms:Decrypt
          Resource: '*'

  environment:
    # for entrypoint and enrichment
    ADAPTORS: ${file(./env.js):ADAPTORS}
    # for api handlers
    APIG_URL:
      {
        'Fn::Join':
          [
            '',
            [
              'https://',
              { Ref: 'HttpApi' },
              '.execute-api.',
              { Ref: 'AWS::Region' },
              '.',
              { Ref: 'AWS::URLSuffix' },
            ],
          ],
      }
    SSM_PATH: ${self:custom.ssmPath}
    BUCKET_DATA: { Ref: BucketData }
    DB_USER: ${file(./env.js):DB_USER}
    DB_NAME: ${file(./env.js):DB_NAME}
    DB_PW: ${file(./env.js):DB_PW}
    DB_PORT: ${file(./env.js):DB_PORT}
    DB_HOST: ${file(./env.js):DB_HOST}

  httpApi:
    metrics: true

functions:
  # --- top-lvl-entrypoint
  triggerEntrypoint:
    handler: src/handlers/triggerEntrypoint.handler
    description: Lambda to launch the adaptor pipeline
    timeout: 30
    events:
      # every new hour
      - schedule: cron(0 * * * ? *)
    environment:
      ADAPTER_QUEUE_URL: { Ref: AdapterQueue }

  # --- adaptors
  triggerAdaptor:
    handler: src/handlers/triggerAdaptor.handler
    description: Lambda which runs adaptors
    timeout: 600
    events:
      - sqs:
          arn:
            Fn::GetAtt:
              - AdapterQueue
              - Arn
          batchSize: 2
          functionResponseType: ReportBatchItemFailures
    environment:
      INFURA_CONNECTION: ${file(./env.js):INFURA_CONNECTION}
      ALCHEMY_CONNECTION_POLYGON: ${file(./env.js):ALCHEMY_CONNECTION_POLYGON}
      ALCHEMY_CONNECTION_ARBITRUM: ${file(./env.js):ALCHEMY_CONNECTION_ARBITRUM}
      XDAI_RPC: ${file(./env.js):XDAI_RPC}
      CRONOS_RPC: ${file(./env.js):CRONOS_RPC}
      TVL_SPIKE_WEBHOOK: ${file(./env.js):TVL_SPIKE_WEBHOOK}
      NEW_YIELDS_WEBHOOK: ${file(./env.js):NEW_YIELDS_WEBHOOK}

  # --- data enrichment
  triggerEnrichment:
    handler: src/handlers/triggerEnrichment.handler
    description: Lambda which runs enrichment process
    timeout: 300
    events:
      # every hour at 20 past
      - schedule: cron(20 * * * ? *)

  # --- stats update
  triggerStats:
    handler: src/handlers/triggerStats.handler
    description: Lambda which updates the stats table
    timeout: 300
    events:
      # at midnight
      - schedule: cron(0 0 * * ? *)

  # --- median insert
  triggerMedian:
    handler: src/handlers/triggerMedian.handler
    description: Lambda which inserts latest value into the median table
    timeout: 300
    events:
      # at midnight
      - schedule: cron(0 0 * * ? *)

  # --- save poolsEnriched as CSV
  triggerCsv:
    handler: src/handlers/triggerCsv.handler
    description: Lambda which dumps poolsEnriched as csv to s3 bucket
    timeout: 300
    events:
      # every hour at 25 past
      - schedule: cron(25 * * * ? *)

  # --- DB Crud operations
  getPools:
    handler: src/handlers/getPools.handler
    description: Lambda for retrieving the latest data for each unique pool
    timeout: 30
    events:
      - httpApi:
          method: get
          path: /simplePools

  getPoolsEnriched:
    handler: src/handlers/getPoolsEnriched.handler
    description: Lambda for retrieving the latest enriched data for each unique pool
    timeout: 20
    events:
      - httpApi:
          method: get
          path: /poolsEnriched
      - httpApi:
          method: get
          path: /pools

  getChart:
    handler: src/handlers/getChart.handler
    description: Lambda for retrieving chart data for a particular pool
    timeout: 20
    events:
      - httpApi:
          method: get
          path: /chart/{pool}

  getMedian:
    handler: src/handlers/getMedian.handler
    description: Lambda for retrieving median data
    timeout: 20
    events:
      - httpApi:
          method: get
          path: /median

  getUrls:
    handler: src/handlers/getUrls.handler
    description: Lambda for retrieving url data
    timeout: 20
    events:
      - httpApi:
          method: get
          path: /urls

  # //////////////////// postgres testing
  getChartPostgres:
    handler: src/handlers/getChartPostgres.handler
    description: Lambda for retrieving chart data for a particular pool
    timeout: 20
    events:
      - httpApi:
          method: get
          path: /chartPostgres/{pool}

  getPoolsEnrichedPostgres:
    handler: src/handlers/getPoolsEnrichedPostgres.handler
    description: Lambda for retrieving enriched data
    timeout: 20
    events:
      - httpApi:
          method: get
          path: /poolsEnrichedPostgres

resources:
  Resources:
    # QUEUES
    # --- queue for adaptor handler
    AdapterQueue:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:service}-${self:custom.stage}-AdapterQueue
        VisibilityTimeout: 660
        # setting this to 9hours
        MessageRetentionPeriod: 32400
        RedrivePolicy:
          deadLetterTargetArn:
            Fn::GetAtt:
              - DeadLetterQueue
              - Arn
          # this params is requried, otherwise cloudformation error
          # means that after 3 failed runs, the message will be moved from the adaptor queue
          # to the DLQ
          maxReceiveCount: 3

    # --- DLQ
    DeadLetterQueue:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:service}-${self:custom.stage}-DeadLetterQueue
        # leaving this at max, 14days, after that msg in there will be deleted
        MessageRetentionPeriod: 1209600

    # --- create bucket
    BucketData:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:service}-${self:custom.stage}-data

    # --- alarm stuff for DLQ
    DlqAlarm:
      Type: AWS::CloudWatch::Alarm
      Properties:
        AlarmName: ${self:service}-${self:custom.stage}-AdapterDLQ
        AlarmDescription: There are failed messages in the dead letter queue.
        Namespace: AWS/SQS
        MetricName: ApproximateNumberOfMessagesVisible
        Dimensions:
          - Name: QueueName
            Value: !GetAtt DeadLetterQueue.QueueName
        Statistic: Sum
        Period: 60
        EvaluationPeriods: 1
        Threshold: 0
        ComparisonOperator: GreaterThanThreshold
        AlarmActions:
          - !Ref DlqAlarmEmail

    DlqAlarmEmail:
      Type: AWS::SNS::Topic
      Properties:
        Subscription:
          - Endpoint: slasher125@protonmail.com
            Protocol: email

custom:
  stage: ${opt:stage, self:provider.stage}
  ssmPath: /${self:service}/serverless/sls-authenticate
  webpack:
    webpackConfig: 'webpack.config.js'
    includeModules: true
    packager: 'npm'
    excludeFiles: src/**/*.test.js
  prune:
    automatic: true
    number: 5

plugins:
  - serverless-webpack
  - serverless-prune-plugin
